{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook trains SPAM classifier for Zenodo records.\n",
    "\n",
    "Run the cells in sequence to train the model on the data. Some steps are optional or used for experimentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sklearn\n",
    "import requests\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "\n",
    "Loads the previously dumped data (TXT). Each line in the file is single record's metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the filename accordingly\n",
    "FILENAME = \"./data/zenodo_open_metadata_17_05_2018.txt\"\n",
    "\n",
    "with open(FILENAME, \"r\") as fp:\n",
    "    data = [json.loads(l) for l in fp.readlines()]\n",
    "print(\"Loaded metadata of {} records\".format(len(data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional step: Manually mark some spammers\n",
    "\n",
    "Next cell allows for manually marking some Users as spammers. Provide User IDs (int) of record owners, which records should be marked as SPAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_owners = {}  # Manually mark some User IDs as SPAMmers\n",
    "for d in data:\n",
    "    owner = d['owners'][0] if d['owners'] else None\n",
    "    if owner in spam_owners and not d['spam']:\n",
    "        d['spam'] = True\n",
    "        \n",
    "spamcnt = Counter([d['spam'] for d in data])\n",
    "print(\"SPAM: {0}, Non-SPAM: {1}\".format(spamcnt[True], spamcnt[False]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "\n",
    "Train the model on the SPAM label. You can experiment with parameters here, and observe the accuracy on the test set (Spam->Spam, Ham->Ham values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [d['spam'] for d in data]\n",
    "X_train_full, X_test_full, y_train, y_test = train_test_split(data, labels, test_size=0.33, random_state=422)\n",
    "def feat_tr(d):\n",
    "    return d['description'] + d['title']\n",
    "\n",
    "X_train = [feat_tr(d) for d in X_train_full]\n",
    "X_test = [feat_tr(d) for d in X_test_full]\n",
    "\n",
    "ngram_range=(1, 1)\n",
    "\n",
    "## Alternatively you can experiment with building a spam vocabulary from the training dataset\n",
    "# X_train_spam = [feat_tr(d) for d in X_train_full if d['spam']]\n",
    "# count_vect = CountVectorizer(ngram_range=ngram_range, max_features=8000)\n",
    "# count_vect.fit_transform(X_train_spam)\n",
    "# vocabulary = count_vect.vocabulary_\n",
    "\n",
    "text_clf = Pipeline([\n",
    "                     ('vect', CountVectorizer(max_features=8000, ngram_range=ngram_range)),\n",
    "                     #('vect', CountVectorizer(vocabulary=vocabulary, ngram_range=ngram_range)),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', MultinomialNB()),\n",
    "                     #('clf', SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, n_iter=5, random_state=42)),\n",
    "                    ])\n",
    "                     \n",
    "text_clf = text_clf.fit(X_train, y_train)\n",
    "y_pred = text_clf.predict(X_test)\n",
    "acc = [(ref, pred) for ref, pred in zip(y_test, y_pred)]\n",
    "c = Counter(acc)\n",
    "print(c)\n",
    "print(\"Spam->Spam: {0:.4f}\".format(c[(True, True)] / (c[(True, True)] + c[(True, False)])))\n",
    "print(\"Ham -> Ham: {0:.4f}\".format(c[(False, False)] / (c[(False, False)] + c[(False, True)])))\n",
    "print(\"Accuracy: {0:.4f}\".format((c[(False, False)] + c[(True, True)] ) / (len(acc))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check accuracy again on the full data.\n",
    "\n",
    "This contains biased because some data was used for training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tr = [feat_tr(d) for d in data]\n",
    "y_pred = text_clf.predict(data_tr)\n",
    "acc = [(ref, pred) for ref, pred in zip(labels, y_pred)]\n",
    "c = Counter(acc)\n",
    "print(c)\n",
    "print(\"Spam->Spam: {0:.4f}\".format(c[(True, True)] / (c[(True, True)] + c[(True, False)])))\n",
    "print(\"Ham -> Ham: {0:.4f}\".format(c[(False, False)] / (c[(False, False)] + c[(False, True)])))\n",
    "print(\"Accuracy: {0:.4f}\".format((c[(False, False)] + c[(True, True)] ) / (len(acc))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate the records\n",
    "\n",
    "The next cell allows you to take a peek at false negatives (i.e.: SPAM which slipped through the filter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "acc = [idx for idx, (ref, pred) in enumerate(zip(labels, y_pred)) if (ref, pred) == (False, True)]\n",
    "spammy_stuff = [(data[idx]['recid'], data[idx]['description']) for idx in acc]\n",
    "\n",
    "for rec in spammy_stuff:\n",
    "    print(rec)\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "joblib.dump(text_clf, '2017_06_18_record_spam.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = [idx for idx, (ref, pred) in enumerate(zip(y_test, y_pred)) if (ref, pred) == (False, True)]\n",
    "spammy_stuff = [(X_test_full[idx]['recid'], X_test_full[idx]['description']) for idx in acc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spammy_stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spams = []\n",
    "for recid, _ in spammy_stuff:\n",
    "    resp = requests.get('https://zenodo.org/record/{0}'.format(recid))\n",
    "    if resp.status_code == 410:\n",
    "        spams.append(recid)\n",
    "print(len(spams), spams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spam_ratio(ground, pred):\n",
    "    tf = [(ref, pred) for ref, pred in zip(y_test, y_pred) if (ref, pred) == (True, False)]\n",
    "    t = [(ref, pred) for ref, pred in zip(y_test, y_pred) if ref == True]\n",
    "    return float(tf) / float(t)\n",
    "score_fun = make_scorer(spam_ratio, greater_is_better=False)\n",
    "\n",
    "text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                                           alpha=1e-3, n_iter=5, random_state=42)),])\n",
    "text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', MultinomialNB()),])\n",
    "parameters = {\n",
    "    'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "    # 'tfidf__use_idf': (True, False),\n",
    "    # 'clf__alpha': (1e-2, 1e-3),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_clf = GridSearchCV(text_clf, parameters, n_jobs=-1)\n",
    "gs_clf = gs_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = gs_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_fun(gs_clf, y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = [(ref, pred) for ref, pred in zip(y_test, y_pred)]\n",
    "Counter(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_clf = GridSearchCV(text_clf, parameters, n_jobs=-1, scoring=score_fun)\n",
    "gs_clf = gs_clf.fit(X_train, y_train)\n",
    "y_pred = gs_clf.predict(y_test)\n",
    "acc = [(ref, pred) for ref, pred in zip(y_test, y_pred)]\n",
    "Counter(acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}